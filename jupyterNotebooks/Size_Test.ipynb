{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing compressed residual size with Huffman\n",
    "In this notebook, we will look at all of test data and see what kind of compression we can achieve on the binarized residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.model import Encoder, Decoder\n",
    "from src.dataset import ResidualDataset\n",
    "from src.HuffmanCompression import HuffmanCoding\n",
    "import pdb\n",
    "from copy import copy\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_enc = '/Users/aditya/Documents/cs348k/Project/NeuralVideoCompression/checkpoint/testing/encoder-epoch_49_iter_40550.pth'\n",
    "checkpoint_dec = '/Users/aditya/Documents/cs348k/Project/NeuralVideoCompression/checkpoint/testing/decoder-epoch_49_iter_40550.pth'\n",
    "enc = Encoder()\n",
    "dec = Decoder()\n",
    "# load both models\n",
    "enc.load_state_dict(torch.load(checkpoint_enc, map_location='cpu'))\n",
    "dec.load_state_dict(torch.load(checkpoint_dec, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset mode: test, length: 21\n"
     ]
    }
   ],
   "source": [
    "train_data_path = '/Users/aditya/Downloads/Video_data/t'\n",
    "device = torch.device('cpu')\n",
    "dSet_train = ResidualDataset(train_data_path, 'test', device)\n",
    "dataset_train = torch.utils.data.DataLoader(dSet_train,\n",
    "                                            batch_size=1, shuffle=True,\n",
    "                                            num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop to get all the reisduals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_out_list = []\n",
    "with torch.no_grad():\n",
    "        for idx, sample in enumerate(dataset_train):\n",
    "            model_input = sample['image']\n",
    "            bin_out = enc(model_input).squeeze().numpy()\n",
    "            bin_out_list.append(bin_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 32, 45, 150)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check it's shape\n",
    "bin_out_arr = ((np.array(bin_out_list) + 1)/2).astype(int)\n",
    "bin_out_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through the tensor, chunk one dimension, build a dict with chunk frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "N,H,W,C = bin_out_arr.shape\n",
    "chunk_size = 30\n",
    "huffmap ={}\n",
    "i=j=k=l=0\n",
    "while i<N:\n",
    "    j=0\n",
    "    while j<H:\n",
    "        k=0\n",
    "        while k<W:\n",
    "            channel = np.split(bin_out_arr[i][j][k][:],chunk_size)\n",
    "            for num in channel:\n",
    "                s =\"\"\n",
    "                for ele in num:\n",
    "                    s += str(ele)\n",
    "                huffmap[s] = huffmap.get(s, 0) + 1\n",
    "            k+=1\n",
    "        j+=1\n",
    "    i+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'11111': 132874,\n",
       " '11101': 25038,\n",
       " '11001': 18611,\n",
       " '01111': 34519,\n",
       " '10000': 33449,\n",
       " '01001': 15128,\n",
       " '10110': 14659,\n",
       " '11110': 34474,\n",
       " '11011': 23747,\n",
       " '00010': 25821,\n",
       " '00110': 19085,\n",
       " '10001': 18046,\n",
       " '11100': 27567,\n",
       " '00000': 117880,\n",
       " '00001': 34678,\n",
       " '01011': 15570,\n",
       " '11010': 15663,\n",
       " '00011': 27672,\n",
       " '01100': 18714,\n",
       " '01000': 25567,\n",
       " '10111': 24391,\n",
       " '10100': 15679,\n",
       " '11000': 27073,\n",
       " '01110': 18128,\n",
       " '10011': 18747,\n",
       " '00100': 24768,\n",
       " '00111': 27454,\n",
       " '10101': 12975,\n",
       " '00101': 15785,\n",
       " '10010': 15180,\n",
       " '01010': 13150,\n",
       " '01101': 15108}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huffmap # print map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's use the huffman function to heapify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'11001': '00000',\n",
       " '01100': '00001',\n",
       " '10011': '00010',\n",
       " '00110': '00011',\n",
       " '11011': '00100',\n",
       " '10111': '00101',\n",
       " '00100': '00110',\n",
       " '11101': '00111',\n",
       " '01000': '01000',\n",
       " '00010': '01001',\n",
       " '10101': '010100',\n",
       " '01010': '010101',\n",
       " '11000': '01011',\n",
       " '00111': '01100',\n",
       " '11100': '01101',\n",
       " '00011': '01110',\n",
       " '10110': '011110',\n",
       " '01101': '011111',\n",
       " '00000': '100',\n",
       " '01001': '101000',\n",
       " '10010': '101001',\n",
       " '01011': '101010',\n",
       " '11010': '101011',\n",
       " '10100': '101100',\n",
       " '00101': '101101',\n",
       " '10000': '10111',\n",
       " '11111': '110',\n",
       " '11110': '11100',\n",
       " '01111': '11101',\n",
       " '00001': '11110',\n",
       " '10001': '111110',\n",
       " '01110': '111111'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = HuffmanCoding()\n",
    "h.make_heap(huffmap)\n",
    "h.merge_nodes()\n",
    "h.make_codes()\n",
    "h.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate entropy, total number of coded bits and avg. code word length along with how close we are to entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total bits needed: 4219563\n",
      "Entropy: 4.628487346668847\n",
      "Avg code work length: 4.651193783068784\n",
      "How close are we to Entropy :0.9951181487035452 \n"
     ]
    }
   ],
   "source": [
    "# all compute\n",
    "# let's now get an idea about number of bits\n",
    "tot_bits = 0\n",
    "entropy =0.\n",
    "avg = 0\n",
    "total_counts = 0.\n",
    "codes = h.codes\n",
    "for key in huffmap.keys():\n",
    "    #pdb.set_trace()\n",
    "    tot_bits += huffmap[key] * len(codes[key])\n",
    "    total_counts += huffmap[key]\n",
    "print(\"Total bits needed: {}\".format(tot_bits))\n",
    "for key in huffmap.keys():\n",
    "    #pdb.set_trace()\n",
    "    avg += huffmap[key] * len(codes[key])/total_counts\n",
    "    p = float(huffmap[key])/total_counts\n",
    "    entropy += -(p * np.log2(p))\n",
    "print(\"Entropy: {}\".format(entropy))\n",
    "print(\"Avg code work length: {}\".format(avg))\n",
    "print(\"How close are we to Entropy :{} \".format(entropy/avg))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of useful stuff. Just test here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 0, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 0],\n",
      "         ...,\n",
      "         [0, 1, 1,  ..., 1, 1, 0],\n",
      "         [1, 1, 1,  ..., 1, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 0, 0],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 0, 1, 1],\n",
      "         [1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 1, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 0, 0, 1],\n",
      "         [1, 0, 0,  ..., 0, 1, 1],\n",
      "         ...,\n",
      "         [1, 0, 0,  ..., 0, 1, 1],\n",
      "         [0, 1, 1,  ..., 0, 1, 0],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 1, 1,  ..., 0, 0, 0],\n",
      "         [0, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 1, 1, 0]],\n",
      "\n",
      "        [[1, 1, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 1, 1],\n",
      "         [0, 0, 0,  ..., 0, 0, 1],\n",
      "         ...,\n",
      "         [1, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 1, 1,  ..., 1, 1, 0],\n",
      "         [0, 1, 1,  ..., 0, 0, 1]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 0, 1, 0],\n",
      "         ...,\n",
      "         [0, 1, 1,  ..., 0, 1, 1],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 1, 1]]], dtype=torch.int32)\n",
      "torch.Size([32, 45, 150])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bin has our desired output. It has shape\n",
    "bin_out.shape\n",
    "bin_out = (bin_out + 1)/2\n",
    "bin_out = bin_out.int()\n",
    "print(bin_out)\n",
    "print(bin_out.shape)\n",
    "np.unique(bin_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's write this to binary residual and compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 110415, '.': 215856, '0': 4424361, 'e': 215856, '+': 215856, ' ': 216205, '\\n': 1450}\n",
      "Compressed\n"
     ]
    }
   ],
   "source": [
    "h = HuffmanCoding()\n",
    "residual_file = 'residual.txt'#\n",
    "output_file = open(residual_file, \"w\")\n",
    "bin_out = (bin_out + 1)//2\n",
    "bin_out = bin_out.astype(int)\n",
    "#pdb.set_trace()\n",
    "for row in bin_out:\n",
    "    #pdb.set_trace()\n",
    "    np.savetxt(output_file, row)\n",
    "h.compress(residual_file, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_out = (bin_out + 1)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_out = bin_out.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1, 1, ..., 1, 1, 0],\n",
       "        [1, 0, 0, ..., 1, 0, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 0, 1, ..., 1, 0, 1],\n",
       "        [1, 1, 0, ..., 1, 0, 0],\n",
       "        [1, 1, 1, ..., 1, 0, 0]],\n",
       "\n",
       "       [[1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 0, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 0, 0],\n",
       "        [1, 1, 1, ..., 1, 1, 0]],\n",
       "\n",
       "       [[0, 1, 1, ..., 1, 1, 1],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 1, 0],\n",
       "        ...,\n",
       "        [1, 1, 0, ..., 0, 1, 0],\n",
       "        [0, 0, 1, ..., 1, 1, 1],\n",
       "        [0, 0, 1, ..., 1, 1, 1]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1, 0, 0, ..., 1, 0, 0],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [0, 1, 1, ..., 1, 0, 0],\n",
       "        [0, 1, 1, ..., 0, 0, 1],\n",
       "        [1, 1, 1, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 1, 1, ..., 0, 0, 1],\n",
       "        [0, 1, 0, ..., 1, 1, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 1],\n",
       "        ...,\n",
       "        [0, 1, 0, ..., 0, 1, 1],\n",
       "        [0, 0, 0, ..., 0, 1, 1],\n",
       "        [0, 0, 0, ..., 0, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 0, 1, ..., 1, 0, 1],\n",
       "        [1, 1, 1, ..., 1, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 1, 1, 0],\n",
       "        [1, 1, 1, ..., 0, 1, 0],\n",
       "        [1, 1, 1, ..., 1, 0, 0]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': '0', '\\n': '100', '0': '101', '1': '11'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do a mixed approach here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed\n"
     ]
    }
   ],
   "source": [
    "h = HuffmanCoding()\n",
    "residual_file = 'residual.txt'#\n",
    "output_file = open(residual_file, \"w\")\n",
    "chunk_size = 5\n",
    "with torch.no_grad():\n",
    "        for idx, sample in enumerate(dataset_train):\n",
    "            model_input = sample['image']\n",
    "            bin_out = enc(model_input).squeeze().numpy()\n",
    "            bin_out = ((bin_out + 1)/2).astype(int)\n",
    "            H,W,C = bin_out.shape\n",
    "            #pdb.set_trace()\n",
    "            j=k=l=0\n",
    "            while j<H:\n",
    "                k=0\n",
    "                while k<W:\n",
    "                    channel = np.split(bin_out[j][k][:],chunk_size)\n",
    "                    for num in channel:\n",
    "                        s = \"\" \n",
    "                        for ele in num:\n",
    "                            s += str(ele)\n",
    "                        #pdb.set_trace()\n",
    "                        output_file.write(s + '\\n')\n",
    "                    k+=1\n",
    "                j+=1\n",
    "h.compress(residual_file, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': '0', '\\n': '10', '0': '11'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
