{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.model import Encoder, Decoder\n",
    "from src.dataset import ResidualDataset\n",
    "from src.HuffmanCompression import HuffmanCoding\n",
    "import pdb\n",
    "from copy import copy\n",
    "import time\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_enc = '../checkpoint/test/encoder-epoch_49_iter_40550.pth'\n",
    "checkpoint_dec = '../checkpoint/test/decoder-epoch_49_iter_40550.pth'\n",
    "enc = Encoder()\n",
    "dec = Decoder()\n",
    "# load both models\n",
    "enc.load_state_dict(torch.load(checkpoint_enc, map_location='cpu'))\n",
    "dec.load_state_dict(torch.load(checkpoint_dec, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = '../data'\n",
    "device = torch.device('cpu')\n",
    "dSet_train = ResidualDataset(train_data_path, 'final_test', device)\n",
    "dataset_train = torch.utils.data.DataLoader(dSet_train,\n",
    "                                            batch_size=1, shuffle=True,\n",
    "                                            num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_out_list = []\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(dataset_train):\n",
    "        model_input = sample['image']\n",
    "        bin_out = enc(model_input).squeeze().numpy()\n",
    "        bin_out_list.append(bin_out)\n",
    "bin_out_arr = ((np.array(bin_out_list) + 1)/2).astype(np.uint8)\n",
    "bin_out_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bin = bin_out_arr[10,:,:,:]\n",
    "test_bin = test_bin[np.newaxis]\n",
    "# test_bin = bin_out_arr\n",
    "print(test_bin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N,C,H,W = test_bin.shape\n",
    "tile_x = 9\n",
    "tile_y = 9\n",
    "huffmap ={}\n",
    "\n",
    "pH = pW = 0\n",
    "if  H % tile_y != 0:\n",
    "    pH = tile_y - H % tile_y\n",
    "if  W % tile_x != 0:\n",
    "    pW = tile_x - W % tile_x\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "test_bin_padded = np.pad(test_bin, ((0,0), (0,0), (0, pH), (0, pW)))\n",
    "# print(test_bin_padded.shape)\n",
    "\n",
    "N, C, H, W = test_bin_padded.shape\n",
    "\n",
    "for n in range(N):\n",
    "    for c in range(C):\n",
    "        for i in range(H // tile_x):\n",
    "            ii = i*tile_y\n",
    "            for j in range(W // tile_y):\n",
    "                jj = j*tile_x\n",
    "                tile = test_bin_padded[n,c, ii:ii+tile_y, jj:jj+tile_x].flatten()\n",
    "                s = \"\"\n",
    "                for ele in tile:\n",
    "                    s += str(ele)\n",
    "                huffmap[s] = huffmap.get(s, 0) + 1\n",
    "                \n",
    "h = HuffmanCoding()\n",
    "h.make_heap(huffmap)\n",
    "h.merge_nodes()\n",
    "h.make_codes()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tot_bits = 0\n",
    "entropy =0.\n",
    "avg = 0\n",
    "total_counts = 0.\n",
    "codes = h.codes\n",
    "tot_bits = 0\n",
    "node_count = 0\n",
    "tree_size = 0\n",
    "for key in huffmap.keys():\n",
    "    tot_bits += huffmap[key] * len(codes[key])\n",
    "    total_counts += huffmap[key]\n",
    "    node_count += 1\n",
    "print(\"Total bits in data: {}, Total counts: {}\".format(N*C*H*W, total_counts))\n",
    "print(\"Total bits needed: {}\".format(tot_bits))\n",
    "for key in huffmap.keys():\n",
    "    avg += huffmap[key] * len(codes[key])/total_counts\n",
    "    p = float(huffmap[key])/total_counts\n",
    "    entropy += -(p * np.log2(p))\n",
    "print(\"Entropy: {}\".format(entropy))\n",
    "print(\"Avg code work length: {}\".format(avg))\n",
    "print(\"How close are we to Entropy :{} \".format(entropy/avg))\n",
    "tree_size = (2 + tile_x*tile_y) * node_count - 1\n",
    "print(\"Node Count: {}, Size of Codes: {}, Compression ratio in general: {}\".format(node_count, tree_size, (tile_x*tile_y)/avg))\n",
    "print(\"compression ratio includes padding: {}\".format(216000/tot_bits))\n",
    "print(\"compression ratio includes padding and tree(per frame): {}\".format(216000/(tot_bits+tree_size)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For N = 86\n",
    "(runtime(s), size of codes(B))\n",
    "tile 5x5  : 25.3414, 20971624\n",
    "tile 7x7  : 21.0083, 20971624\n",
    "tile 9x9  : 19.9685, 10485872\n",
    "tile 11x11: 19.2659, 5242984\n",
    "tile 13x13: 17.6710, 5242984\n",
    "    \n",
    "For N = 1\n",
    "tile 5x5  : 0.3230, 295016, 1.9522\n",
    "tile 7x7  : 0.2490, 147568, 3.6048\n",
    "tile 9x9  : 0.2300, 73832, 6.9171\n",
    "tile 11x11: 0.2236, 73832, 8.8152\n",
    "tile 13x13: 0.2131, 73832, 13.2923"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
